# facialrecognition
Using a CNN with residual networks in Pytorch to predict different facial emotions from the FER-2013 dataset; used the Resnet-18 model from Pytorch

# Data Loading and Model
The first thing I did in creating my model was loading and transforming the data from the FER 2013 dataset. I used the ImageFolder module in Pytorch to load in the data, and I used several different transforms to get the data into tensor form and also to make the data grayscale, segment the images, and normalize the data based on the Resnet-18 model. From there, I split the data into training and validation sets, and created data loaders for each of the data sets so that I could pass data in as batches and reshuffle the data in between epochs. Instead of building my own residual network model, I used the built-in Pytorch Resnet-18 model, modifying the last layer of the fully-connected neural network to output 7 features for the 7 different emotions being classified. 

# Training and Evaluating the model
Next, I had to train the model. Like other CNN projects, I stuck with the cross entropy loss function, the Adam optimizer, and this time, I added a learning rate of 0.001 and a learning rate scheduler after playing around with different features and the result of my model. I ran the model through the training loop using similar implementation steps to a regular CNN (optimize, pass images through model, loss function for images and outputs, etc.) before testing the model on the validation data for five epochs. Here, I disabled gradient calculations before running the model and computing the loss, and comparing each predicted emotion with the labeled emotion to see the percentage guessed correctly. Finally, I evaluated the model using the actual test data and got an accuracy of ~61%. Additionally, the program was very computationally intensive, with each run through the model taking around 20-30 minutes on my laptop.

# Conclusion
Although the accuracy was not as high as I would've liked it to be, I learned a lot from this project. I tried new ways to tweak my model to try and improve the accuracy (learning rate scheduling, different optimizers, etc.) and I was able to learn more about the ResNet model and the architecture behind it. Additionally, this project was interesting since I also had to consider implementing a model that also did not take too long to run, which meant doing things like increasing the learning rate or decreasing the number of epochs, since the ResNet model took a lot longer to run than the models I had implemented before. Overall, considering facial emotions are a lot harder and more ambiguous to classify when compared to things like house numbers and handwritten digits, I'd say that 61% is not the worst outcome, although I would like to improve this accuracy significantly in the future.
